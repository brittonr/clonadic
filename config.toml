[server]
host = "localhost"
port = 8080

[llm]
# Provider: "ollama", "openai", or "anthropic"
provider = "ollama"
# Base URL for Ollama or custom OpenAI-compatible endpoints
base_url = "http://localhost:11434"
# API key for OpenAI or Anthropic (not needed for Ollama)
# api_key = "sk-..."
model = "qwen2.5:3b"
temperature = 0.0

[stats]
tokens_per_operation = 300
cost_per_operation = 0.003

[grid]
default_rows = 20
default_cols = 10
